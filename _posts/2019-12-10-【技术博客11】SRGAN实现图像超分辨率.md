---
layout: post
title: 【技术博客11】SRGAN实现图像超分辨率
date: 2019-12-02 12:00
---

作者：王镇

## 引言

这是2017年发表在CVPR上的一篇文章，原文叫做"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network"。图像超分辨率（Image Super-Resolution），顾名思义，即从低分辨率的图像获得高分辨率的图像。传统的CNN网络基于最小化像素级损失函数的方法，尽管在PSNR (peak signal- to-noise ratio) 和SSIM (structural similarity) 这些客观的图像评价指标上得到很好的结果，但它们往往缺乏高频细节而无法在感官视觉上达到满意的程度。因此本文作者提出了SRGAN，一种将GAN应用于图像超分辨率领域的方法，第一次考虑了人类的视觉满意度。


## 一 主要贡献

针对传统超分辨中存在结果过平滑问题，在PSNR和SSIM评价指标上能得到很好的结果，但图像细节显示依旧较差，利用对抗网络结构的方法，得到了视觉特性上较好结果，本文主要贡献如下：

- 建立了使用PSNR和SSIM为评价标准的SRResNet，对图像进行放大4倍，取得了最好的测试结果。
- 提出了SRGAN网络，该网络结构根据生成对抗网络结构提出了一种新的视觉损失函数（perceptual loss），利用VGG的网络特征作为内容损失函数（content loss），代替了之前的MSE损失函数。
- 对生成的图像进行MOS (mean-opinion-score) 评价。
 

## 二 网络结构

![](https://imgbed.momodel.cn/201919102159-Z.png)

![](https://imgbed.momodel.cn/201919102200-Z.png)

图 1 Architecture of Generator and Discriminator Network with corresponding kernel size (k), number of feature maps(n) and stride (s) indicated for each convolutional layer.

该模型的网络结构分为生成网络结构和辨别网络结构。生成网络结构基于Resnet网络结构，用于生成超分辨率的图像。辨别网络结构借鉴了VGG网络结构，用于辨别生成器生成的图片与原始图片的相似度。


## 三 损失函数

### 3.1 训练生成器和判别器

对给定的HR图像进行降采样得到LR图像，将LR图像作为输入，训练生成器 $G_{θ_G}$，生成对应的HR图像，该训练过程与训练前馈CNN类似，对网络参数$ \hat θ_G $进行优化，如下：

![](https://imgbed.momodel.cn/201919102202-E.png)
公式（1）

其中$ l^{SR} $为视觉损失函数，后面将详细介绍。

文中定义的判别器为$D_{θ_G}$，对生成器和判别器交替优化如下式子：

![](https://imgbed.momodel.cn/201919102203-V.png)
公式（2）

### 3.2 感知损失函数（perceptual loss function）

本文定义的感知损失函数是本文算法性能的关键，它充分考虑了与人感知相关的图像特征。感知损失函数包括了内容损失（content loss）和对抗损失（adversarial loss）。

![](https://imgbed.momodel.cn/201919102204-2.png)
公式（3）

#### 3.2.1 内容损失

在图像超分辨率领域，最常被用来最优化的损失函数是基于像素的MSE (mean squared error)。尽管它能取得很高的PSNR，但由于它丢失了图像的高频特征，在图像的纹理方面并不能取得满意的结果。MSE用公式表示如下：

![](https://imgbed.momodel.cn/201919102204-W.png)
公式（4）

文中对内容损失函数进行了改进，基于pre-trained 19层 VGG网络的ReLU激活层定义了VGG loss：

![](https://imgbed.momodel.cn/201919102204-Y.png)
公式（5）

Φ表示在VGG19网络中第i个最大池化层之前的第j个卷积层(在激活层之后)的特征图，VGG loss是重构图像和参考图像的特征图的欧式距离，上式中的$W_{i,j}$和$H_{i,j}$表示特征图的维度。

#### 3.2.2 对抗损失

文中将GAN中生成器对视觉损失的影响通过对抗损失来体现，这部分损失函数使我们的网络通过“欺骗”判别器从而生成更接近自然图像的图像。

![](https://imgbed.momodel.cn/201919102209-W.png)
公式（6）

其中![](https://imgbed.momodel.cn/201919102209-g.png)表示判别器将生成器生成的图像![](https://imgbed.momodel.cn/201919102210-k.png)与HR原图判别为同一图像的概率。


## 四 结果分析

### 4.1 MOS评价

MOS是一种主观评价方式，文中作者让26个评价者对不同图像进行从1到5打分（原HR图为5分），取他们的均值作为该幅图像的MOS值。最终，各方法生成的SR图及原图的MOS评价结果如下：

![](https://imgbed.momodel.cn/201919102210-a.png)
图 2 MOS评价结果

从上图可以发现，SRGAN方法生成的SR图MOS值要远好于传统方法，仅次于原HR图。

### 4.2 定量分析

利用不同的VGG 层作为特征的损失函数性能比较：

![](https://imgbed.momodel.cn/201919102211-B.png)
图 3 不同VGG层性能比较

VGG22：表示第2个池化层之前的第2个卷积层，浅层图像特征；VGG54：表示第5个池化层之前的第4个卷积层，深层图像特征（相应地有更多的图像内容信息）。

从上表可见，SRResNet-MSE取得最高的PSNR和SSIM，而SRGAN取得较好的MOS。从下图可以发现，深层图像特征可取得更好的纹理特征。

![](https://imgbed.momodel.cn/201919102211-u.png)
图 4 各方法重建结果示例

![](https://imgbed.momodel.cn/201919102211-4.png)
图 5 示例2


## 五 总结

作者复现了追求PSNR的SRResNet，并分析了该方法的缺陷（图像纹理方面表现得不够好，不具备真实感），因此，作者提出了追求图像内容的SRGAN，并提出了一种MOS的评价方法。最终，相较其他方法，SRGAN取得了更具真实感的重建结果。

**项目源码地址：**[https://momodel.cn/explore/5dd13c5024d57fa935c71de9?type=app](https://momodel.cn/explore/5dd13c5024d57fa935c71de9?type=app)


## 六 参考文献

1. Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero, etc. [https://arxiv.org/pdf/1609.04802v5.pdf](https://arxiv.org/pdf/1609.04802v5.pdf)
1. DaneAI. [https://blog.csdn.net/happyday_d/article/details/85462378](https://blog.csdn.net/happyday_d/article/details/85462378)
1. [https://github.com/leftthomas/SRGAN](https://github.com/leftthomas/SRGAN)

