---
layout: post
title: 【技术博客09】多标准中文分词 Multi-Criteria-CWS
date: 2019-11-17 12:00
---

作者：宋彤彤

自然语言处理（NLP）是人工智能中很重要且具有挑战性的方向，而自然语言处理的第一步就是分词，分词的效果直接决定和影响后续工作的效率。我们生活和工作中每天都产生着大量的中文数据，由于中文和英文在词句方面有很大的不同，比如英文单词之间以空格作为自然分界符，而中文只是字、句和段能通过明显的分界符来简单划界，“词”和“词组”边界模糊，中文分词相对复杂和困难。所以我们来讨论一下中文分词（Chinese Word Segmentation，CWS）。

## 1. 中文分词现状
中文分词指的是讲一个汉字序列切分成一个一个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。现有的分词方法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。

- 基于字符串匹配的分词方法又叫机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大”的机器词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。常用的字符串匹配方法有如下几种：正向最大匹配法（从左到右）；逆向最大匹配法（从右到左）；最小切分（每一句中切出的词数最小）；双向最大匹配（进行从左到右、从右到左两次扫描）。这类算法的优点是速度快，时间复杂度可以保持在 O(n)，实现简单，效果尚可；但对歧义和未登录词处理效果不佳。
- 基于理解的分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。
- 基于统计的分词方法是在给定大量已经分词的文本的前提下，利用统计机器学习模型学习词语切分的规律（称为训练），从而实现对未知文本的切分。例如最大概率分词方法和最大熵分词方法等。随着大规模语料库的建立，统计机器学习方法的研究和发展，基于统计的中文分词方法渐渐成为了主流方法。主要的统计模型有：N元文法模型（N-gram），隐马尔可夫模型（Hidden Markov Model，HMM），最大熵模型（ME），条件随机场模型（Conditional Random Fields，CRF）等。在实际的应用中，基于统计的分词系统都需要使用分词词典来进行字符串匹配分词，同时使用统计方法识别一些新词，即将字符串频率统计和字符串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。

## 2. Multi-Criteria-CWS
基于统计的分词方法建立在已有的大量已分词文本，即语料库的基础上。为了做出实用的分词工具，不光需要高效的算法，大规模语料库也必不可少。对于缺乏经费的研究团队和个人，往往只能得到 sighan2005 等屈指可数的几个小型语料库。而且这些语料库的标注规范还互不兼容，无法混合起来训练。
  
![WX20191111-134703@2x.png](https://cdn.nlark.com/yuque/0/2019/png/375466/1573456832134-b9028a53-1703-4130-927d-9d9417bd0baf.png#align=left&display=inline&height=171&name=WX20191111-134703%402x.png&originHeight=384&originWidth=718&search=&size=67809&status=done&width=319)

已经有团队开始研究如何利用多方语料库来联合学习中文分词，比如 Chen 等人 2017 年精心设计的对抗神经网络，针对每个语料库提取分词标准相关或无关的特征，但表现不理想。再就是接下来要介绍的 Han He 等人 2018 年提出的方案：受谷歌多语种翻译系统启发，利用工程思想，用标签标识不同标准的数据集，这样就可以识别出自哪个标准的数据集，通过不同语料库之间的迁移学习提升模型的性能，同时输出多标准的分词结果。
    
![WX20191111-134737@2x.png](https://cdn.nlark.com/yuque/0/2019/png/375466/1573456856207-aab3ea31-bce8-4766-967a-a54f03a720d0.png#align=left&display=inline&height=207&name=WX20191111-134737%402x.png&originHeight=470&originWidth=740&search=&size=87820&status=done&width=326)

## 3. 实验及结果
训练用到的模型是大家熟悉的 Bi-LSTM + CRF。在具体联合训练中将引入的两个人工标识符视作普通字符，不必人工区分句子的来源。这两个人工标识符会提示 RNN 这个句子属于哪种分词标准，使其为每个字符生成的 contexual representation 都受到该分词标准的影响。
      
测试的时候，这两个人工标识符起到指定所需分词标准的作用，但并不计入准确率的计算。
    
论文在标准的 sighan2005 和 sighan2008 上做了实验，没有针对性调参的情况下依然取得了更高的成绩（当时设备条件限制，所有数据集上都用了同一套超参数）。所有分值都通过了官方评测脚本的验算。下图的 baseline 是在各个语料库上单独训练的结果，+naive 是合并预料却不加标识符的结果，+multi 是论文中联合训练方案的结果。
    
![WX20191111-145431@2x.png](https://cdn.nlark.com/yuque/0/2019/png/375466/1573456885216-14a7c971-716e-4e93-981c-b8729ae0e638.png#align=left&display=inline&height=351&name=WX20191111-145431%402x.png&originHeight=736&originWidth=738&search=&size=314699&status=done&width=352)

该试验使用的特征是极小的，仅仅是字符和 bigram。如果像最近流行的做法加入 12 个 nagram、词典特征（word embedding）,可能还会有进一步提升。但论文中心是一个简单的多标准分词方案，主打精简高效，并非追求高分胜过效率，所以没有采用这些特征工程的手段。在 sighan2008 上的实验及结果在这里不在赘述。

## 4. 总结
这是一种简单的多标注中文分词解决方案，可以在不增加模型复杂度的情况下联合多个语料库训练单个模型。该方案虽然简单，但的确带来了显著的性能提升（特别是对于小数据集如 WTB）。但特别大的数据集收益很小或无法受益（如 MSR），留作未来研究。这里我们提供该文章的项目地址和一些参考资料，感兴趣的同学可以进一步探索。

**项目地址：**[https://momodel.cn/workspace/5dc9114b269cf99a59565610?type=app](https://momodel.cn/workspace/5dc9114b269cf99a59565610?type=app)
**

## 5. 参考资料
+ 博客：[http://www.hankcs.com/nlp/segment/multi-criteria-cws.html#respond](http://www.hankcs.com/nlp/segment/multi-criteria-cws.html#respond)
+ 博客：[https://www.cnblogs.com/shona/p/11540353.html](https://www.cnblogs.com/shona/p/11540353.html)
+ 博客：[http://www.360doc.com/content/19/0305/12/99071_819341146.shtml](http://www.360doc.com/content/19/0305/12/99071_819341146.shtml)
+ 博客：[https://blog.csdn.net/qq_26598445/article/details/81298456](https://blog.csdn.net/qq_26598445/article/details/81298456)
+ 论文：Effective Neural Solution for Multi-Criteria Word Segmentation, 2018，[https://arxiv.org/abs/1712.02856](https://arxiv.org/abs/1712.02856)

---
## 关于我们
**Mo**（网址：[**https://**](https://momodel.cn)[**momodel.cn**](https://momodel.cn)）是一个支持 Python 的**人工智能在线建模平台**，能帮助你快速开发、训练并部署模型。

目前团队每两周（周六）在杭州举办线下沙龙，进行机器学习相关论文分享与学术交流。希望能汇聚来自各行各业对人工智能感兴趣的朋友，不断交流共同成长，推动人工智能民主化、应用普及化。

![image.png](https://cdn.nlark.com/yuque/0/2019/png/307794/1560565564936-bcd9ec1e-8e47-4373-ba0d-1ab3a696aee4.png#align=left&display=inline&height=175&name=image.png&originHeight=349&originWidth=720&search=&size=170790&status=done&width=360)

